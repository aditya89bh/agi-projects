{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMdX8ZdeUZnmdkOMhvAGoYy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aditya89bh/agi-projects/blob/main/moe_expert_choice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6ilhMHzUgeNE",
        "outputId": "9ed5b411-effe-489d-ca81-3fe005b0fbbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Sparse MoE with Expert Choice Routing...\n",
            "Using device: cuda\n",
            "Model has 735007 parameters\n",
            "Epoch 0, Batch 0, Loss: 198.5426, Main: 14.2226, Load Balance: 184.319992\n",
            "Epoch 0, Batch 10, Loss: 190.1731, Main: 5.8531, Load Balance: 184.319992\n",
            "Epoch 0, Batch 20, Loss: 188.0149, Main: 3.6949, Load Balance: 184.319992\n",
            "Epoch 0, Batch 30, Loss: 187.1446, Main: 2.8246, Load Balance: 184.319992\n",
            "Epoch 0, Batch 40, Loss: 186.8057, Main: 2.4857, Load Balance: 184.319992\n",
            "Epoch 0, Batch 50, Loss: 186.5255, Main: 2.2055, Load Balance: 184.319992\n",
            "Epoch 0, Batch 60, Loss: 186.4806, Main: 2.1606, Load Balance: 184.319992\n",
            "Epoch 0, Batch 70, Loss: 186.5063, Main: 2.1863, Load Balance: 184.319992\n",
            "Epoch 0, Batch 80, Loss: 186.3682, Main: 2.0482, Load Balance: 184.319992\n",
            "Epoch 0, Batch 90, Loss: 186.4210, Main: 2.1010, Load Balance: 184.319992\n",
            "Epoch 0, Batch 100, Loss: 186.3587, Main: 2.0387, Load Balance: 184.319992\n",
            "Epoch 0, Batch 110, Loss: 186.3034, Main: 1.9834, Load Balance: 184.319992\n",
            "Epoch 0, Batch 120, Loss: 186.2599, Main: 1.9399, Load Balance: 184.319992\n",
            "Epoch 0, Batch 130, Loss: 186.2631, Main: 1.9431, Load Balance: 184.319992\n",
            "Epoch 0, Batch 140, Loss: 186.1771, Main: 1.8571, Load Balance: 184.319992\n",
            "Epoch 0, Batch 150, Loss: 186.2666, Main: 1.9466, Load Balance: 184.319992\n",
            "Epoch 0, Batch 160, Loss: 186.1188, Main: 1.7988, Load Balance: 184.319992\n",
            "Epoch 0, Batch 170, Loss: 186.0945, Main: 1.7745, Load Balance: 184.319992\n",
            "Epoch 0, Batch 180, Loss: 186.1711, Main: 1.8511, Load Balance: 184.319992\n",
            "Epoch 0, Batch 190, Loss: 186.0689, Main: 1.7489, Load Balance: 184.319992\n",
            "Epoch 0, Batch 200, Loss: 185.9557, Main: 1.6357, Load Balance: 184.319992\n",
            "Epoch 0, Batch 210, Loss: 186.0638, Main: 1.7438, Load Balance: 184.319992\n",
            "Epoch 0, Batch 220, Loss: 185.9480, Main: 1.6280, Load Balance: 184.319992\n",
            "Epoch 0, Batch 230, Loss: 185.9385, Main: 1.6185, Load Balance: 184.319992\n",
            "Epoch 0, Batch 240, Loss: 186.0149, Main: 1.6949, Load Balance: 184.319992\n",
            "Epoch 0, Batch 250, Loss: 185.9771, Main: 1.6571, Load Balance: 184.319992\n",
            "Epoch 0, Batch 260, Loss: 185.9774, Main: 1.6574, Load Balance: 184.319992\n",
            "Epoch 0, Batch 270, Loss: 185.9563, Main: 1.6363, Load Balance: 184.319992\n",
            "Epoch 0, Batch 280, Loss: 185.8627, Main: 1.5427, Load Balance: 184.319992\n",
            "Epoch 0, Batch 290, Loss: 185.9682, Main: 1.6482, Load Balance: 184.319992\n",
            "Epoch 0, Batch 300, Loss: 185.8200, Main: 1.5000, Load Balance: 184.319992\n",
            "Epoch 0, Batch 310, Loss: 185.8820, Main: 1.5620, Load Balance: 184.319992\n",
            "Epoch 0, Batch 320, Loss: 185.8607, Main: 1.5407, Load Balance: 184.319992\n",
            "Epoch 0, Batch 330, Loss: 185.8682, Main: 1.5482, Load Balance: 184.319992\n",
            "Epoch 0, Batch 340, Loss: 185.9445, Main: 1.6245, Load Balance: 184.319992\n",
            "Epoch 0, Batch 350, Loss: 185.8136, Main: 1.4936, Load Balance: 184.319992\n",
            "Epoch 0, Batch 360, Loss: 185.8278, Main: 1.5078, Load Balance: 184.319992\n",
            "Epoch 0, Batch 370, Loss: 185.8263, Main: 1.5063, Load Balance: 184.319992\n",
            "Epoch 0, Batch 380, Loss: 185.7788, Main: 1.4588, Load Balance: 184.319992\n",
            "Epoch 0, Batch 390, Loss: 185.8757, Main: 1.5557, Load Balance: 184.319992\n",
            "Epoch 0, Batch 400, Loss: 185.9008, Main: 1.5808, Load Balance: 184.319992\n",
            "Epoch 0, Batch 410, Loss: 185.8382, Main: 1.5182, Load Balance: 184.319992\n",
            "Epoch 0, Batch 420, Loss: 185.7985, Main: 1.4785, Load Balance: 184.319992\n",
            "Epoch 0, Batch 430, Loss: 185.8605, Main: 1.5405, Load Balance: 184.319992\n",
            "Epoch 0, Batch 440, Loss: 185.9113, Main: 1.5913, Load Balance: 184.319992\n",
            "Epoch 0, Batch 450, Loss: 185.8041, Main: 1.4841, Load Balance: 184.319992\n",
            "Epoch 0, Batch 460, Loss: 185.8234, Main: 1.5034, Load Balance: 184.319992\n",
            "Epoch 0, Batch 470, Loss: 185.8182, Main: 1.4982, Load Balance: 184.319992\n",
            "Epoch 0, Batch 480, Loss: 185.7788, Main: 1.4588, Load Balance: 184.319992\n",
            "Epoch 0, Batch 490, Loss: 185.8181, Main: 1.4981, Load Balance: 184.319992\n",
            "Epoch 0, Batch 500, Loss: 185.8027, Main: 1.4828, Load Balance: 184.319992\n",
            "Epoch 0, Batch 510, Loss: 185.7327, Main: 1.4127, Load Balance: 184.319992\n",
            "Epoch 0, Batch 520, Loss: 185.7427, Main: 1.4227, Load Balance: 184.319992\n",
            "Epoch 0, Batch 530, Loss: 185.8181, Main: 1.4981, Load Balance: 184.319992\n",
            "Epoch 0, Batch 540, Loss: 185.7183, Main: 1.3983, Load Balance: 184.319992\n",
            "Epoch 0, Batch 550, Loss: 185.6822, Main: 1.3622, Load Balance: 184.319992\n",
            "Epoch 0, Batch 560, Loss: 185.7938, Main: 1.4738, Load Balance: 184.319992\n",
            "Epoch 0, Batch 570, Loss: 185.7516, Main: 1.4316, Load Balance: 184.319992\n",
            "Epoch 0, Batch 580, Loss: 185.7224, Main: 1.4024, Load Balance: 184.319992\n",
            "Epoch 0, Batch 590, Loss: 185.7505, Main: 1.4306, Load Balance: 184.319992\n",
            "Epoch 0, Batch 600, Loss: 185.7520, Main: 1.4320, Load Balance: 184.319992\n",
            "Epoch 0 completed. Avg Loss: 1.8812, Avg LB Loss: 184.151015\n",
            "Epoch 1, Batch 0, Loss: 185.7703, Main: 1.4504, Load Balance: 184.319992\n",
            "Epoch 1, Batch 10, Loss: 185.7523, Main: 1.4323, Load Balance: 184.319992\n",
            "Epoch 1, Batch 20, Loss: 185.6942, Main: 1.3742, Load Balance: 184.319992\n",
            "Epoch 1, Batch 30, Loss: 185.7470, Main: 1.4270, Load Balance: 184.319992\n",
            "Epoch 1, Batch 40, Loss: 185.7082, Main: 1.3882, Load Balance: 184.319992\n",
            "Epoch 1, Batch 50, Loss: 185.7934, Main: 1.4734, Load Balance: 184.319992\n",
            "Epoch 1, Batch 60, Loss: 185.6667, Main: 1.3467, Load Balance: 184.319992\n",
            "Epoch 1, Batch 70, Loss: 185.7832, Main: 1.4632, Load Balance: 184.319992\n",
            "Epoch 1, Batch 80, Loss: 185.7521, Main: 1.4321, Load Balance: 184.319992\n",
            "Epoch 1, Batch 90, Loss: 185.7760, Main: 1.4560, Load Balance: 184.319992\n",
            "Epoch 1, Batch 100, Loss: 185.6947, Main: 1.3747, Load Balance: 184.319992\n",
            "Epoch 1, Batch 110, Loss: 185.7205, Main: 1.4005, Load Balance: 184.319992\n",
            "Epoch 1, Batch 120, Loss: 185.7275, Main: 1.4075, Load Balance: 184.319992\n",
            "Epoch 1, Batch 130, Loss: 185.6891, Main: 1.3691, Load Balance: 184.319992\n",
            "Epoch 1, Batch 140, Loss: 185.6737, Main: 1.3537, Load Balance: 184.319992\n",
            "Epoch 1, Batch 150, Loss: 185.6475, Main: 1.3275, Load Balance: 184.319992\n",
            "Epoch 1, Batch 160, Loss: 185.6863, Main: 1.3663, Load Balance: 184.319992\n",
            "Epoch 1, Batch 170, Loss: 185.7359, Main: 1.4159, Load Balance: 184.319992\n",
            "Epoch 1, Batch 180, Loss: 185.7015, Main: 1.3815, Load Balance: 184.319992\n",
            "Epoch 1, Batch 190, Loss: 185.7013, Main: 1.3813, Load Balance: 184.319992\n",
            "Epoch 1, Batch 200, Loss: 185.6981, Main: 1.3781, Load Balance: 184.319992\n",
            "Epoch 1, Batch 210, Loss: 185.7363, Main: 1.4163, Load Balance: 184.319992\n",
            "Epoch 1, Batch 220, Loss: 185.6841, Main: 1.3641, Load Balance: 184.319992\n",
            "Epoch 1, Batch 230, Loss: 185.6963, Main: 1.3763, Load Balance: 184.319992\n",
            "Epoch 1, Batch 240, Loss: 185.6800, Main: 1.3600, Load Balance: 184.319992\n",
            "Epoch 1, Batch 250, Loss: 185.6912, Main: 1.3712, Load Balance: 184.319992\n",
            "Epoch 1, Batch 260, Loss: 185.6400, Main: 1.3200, Load Balance: 184.319992\n",
            "Epoch 1, Batch 270, Loss: 185.6848, Main: 1.3648, Load Balance: 184.319992\n",
            "Epoch 1, Batch 280, Loss: 185.6155, Main: 1.2955, Load Balance: 184.319992\n",
            "Epoch 1, Batch 290, Loss: 185.6830, Main: 1.3630, Load Balance: 184.319992\n",
            "Epoch 1, Batch 300, Loss: 185.7175, Main: 1.3975, Load Balance: 184.319992\n",
            "Epoch 1, Batch 310, Loss: 185.7188, Main: 1.3988, Load Balance: 184.319992\n",
            "Epoch 1, Batch 320, Loss: 185.6592, Main: 1.3392, Load Balance: 184.319992\n",
            "Epoch 1, Batch 330, Loss: 185.6440, Main: 1.3240, Load Balance: 184.319992\n",
            "Epoch 1, Batch 340, Loss: 185.6467, Main: 1.3267, Load Balance: 184.319992\n",
            "Epoch 1, Batch 350, Loss: 185.6556, Main: 1.3356, Load Balance: 184.319992\n",
            "Epoch 1, Batch 360, Loss: 185.7137, Main: 1.3938, Load Balance: 184.319992\n",
            "Epoch 1, Batch 370, Loss: 185.6561, Main: 1.3361, Load Balance: 184.319992\n",
            "Epoch 1, Batch 380, Loss: 185.6576, Main: 1.3376, Load Balance: 184.319992\n",
            "Epoch 1, Batch 390, Loss: 185.6483, Main: 1.3283, Load Balance: 184.319992\n",
            "Epoch 1, Batch 400, Loss: 185.6755, Main: 1.3555, Load Balance: 184.319992\n",
            "Epoch 1, Batch 410, Loss: 185.6692, Main: 1.3492, Load Balance: 184.319992\n",
            "Epoch 1, Batch 420, Loss: 185.7565, Main: 1.4365, Load Balance: 184.319992\n",
            "Epoch 1, Batch 430, Loss: 185.6444, Main: 1.3244, Load Balance: 184.319992\n",
            "Epoch 1, Batch 440, Loss: 185.6529, Main: 1.3329, Load Balance: 184.319992\n",
            "Epoch 1, Batch 450, Loss: 185.7150, Main: 1.3950, Load Balance: 184.319992\n",
            "Epoch 1, Batch 460, Loss: 185.6835, Main: 1.3635, Load Balance: 184.319992\n",
            "Epoch 1, Batch 470, Loss: 185.6523, Main: 1.3323, Load Balance: 184.319992\n",
            "Epoch 1, Batch 480, Loss: 185.6619, Main: 1.3419, Load Balance: 184.319992\n",
            "Epoch 1, Batch 490, Loss: 185.6122, Main: 1.2922, Load Balance: 184.319992\n",
            "Epoch 1, Batch 500, Loss: 185.6288, Main: 1.3088, Load Balance: 184.319992\n",
            "Epoch 1, Batch 510, Loss: 185.6639, Main: 1.3439, Load Balance: 184.319992\n",
            "Epoch 1, Batch 520, Loss: 185.6619, Main: 1.3419, Load Balance: 184.319992\n",
            "Epoch 1, Batch 530, Loss: 185.6222, Main: 1.3022, Load Balance: 184.319992\n",
            "Epoch 1, Batch 540, Loss: 185.7189, Main: 1.3989, Load Balance: 184.319992\n",
            "Epoch 1, Batch 550, Loss: 185.6313, Main: 1.3113, Load Balance: 184.319992\n",
            "Epoch 1, Batch 560, Loss: 185.6481, Main: 1.3281, Load Balance: 184.319992\n",
            "Epoch 1, Batch 570, Loss: 185.6481, Main: 1.3281, Load Balance: 184.319992\n",
            "Epoch 1, Batch 580, Loss: 185.6567, Main: 1.3367, Load Balance: 184.319992\n",
            "Epoch 1, Batch 590, Loss: 185.6466, Main: 1.3266, Load Balance: 184.319992\n",
            "Epoch 1, Batch 600, Loss: 185.7266, Main: 1.4067, Load Balance: 184.319992\n",
            "Epoch 1 completed. Avg Loss: 1.3669, Avg LB Loss: 184.151015\n",
            "Epoch 2, Batch 0, Loss: 185.5671, Main: 1.2471, Load Balance: 184.319992\n",
            "Epoch 2, Batch 10, Loss: 185.6255, Main: 1.3055, Load Balance: 184.319992\n",
            "Epoch 2, Batch 20, Loss: 185.6048, Main: 1.2848, Load Balance: 184.319992\n",
            "Epoch 2, Batch 30, Loss: 185.6265, Main: 1.3065, Load Balance: 184.319992\n",
            "Epoch 2, Batch 40, Loss: 185.7216, Main: 1.4016, Load Balance: 184.319992\n",
            "Epoch 2, Batch 50, Loss: 185.6028, Main: 1.2828, Load Balance: 184.319992\n",
            "Epoch 2, Batch 60, Loss: 185.6910, Main: 1.3710, Load Balance: 184.319992\n",
            "Epoch 2, Batch 70, Loss: 185.7365, Main: 1.4165, Load Balance: 184.319992\n",
            "Epoch 2, Batch 80, Loss: 185.6802, Main: 1.3602, Load Balance: 184.319992\n",
            "Epoch 2, Batch 90, Loss: 185.6060, Main: 1.2860, Load Balance: 184.319992\n",
            "Epoch 2, Batch 100, Loss: 185.6393, Main: 1.3193, Load Balance: 184.319992\n",
            "Epoch 2, Batch 110, Loss: 185.5842, Main: 1.2642, Load Balance: 184.319992\n",
            "Epoch 2, Batch 120, Loss: 185.6020, Main: 1.2820, Load Balance: 184.319992\n",
            "Epoch 2, Batch 130, Loss: 185.6227, Main: 1.3027, Load Balance: 184.319992\n",
            "Epoch 2, Batch 140, Loss: 185.6792, Main: 1.3592, Load Balance: 184.319992\n",
            "Epoch 2, Batch 150, Loss: 185.6877, Main: 1.3678, Load Balance: 184.319992\n",
            "Epoch 2, Batch 160, Loss: 185.6465, Main: 1.3265, Load Balance: 184.319992\n",
            "Epoch 2, Batch 170, Loss: 185.6424, Main: 1.3224, Load Balance: 184.319992\n",
            "Epoch 2, Batch 180, Loss: 185.6416, Main: 1.3216, Load Balance: 184.319992\n",
            "Epoch 2, Batch 190, Loss: 185.6520, Main: 1.3320, Load Balance: 184.319992\n",
            "Epoch 2, Batch 200, Loss: 185.6416, Main: 1.3216, Load Balance: 184.319992\n",
            "Epoch 2, Batch 210, Loss: 185.5940, Main: 1.2740, Load Balance: 184.319992\n",
            "Epoch 2, Batch 220, Loss: 185.6702, Main: 1.3502, Load Balance: 184.319992\n",
            "Epoch 2, Batch 230, Loss: 185.5922, Main: 1.2722, Load Balance: 184.319992\n",
            "Epoch 2, Batch 240, Loss: 185.6224, Main: 1.3024, Load Balance: 184.319992\n",
            "Epoch 2, Batch 250, Loss: 185.6257, Main: 1.3057, Load Balance: 184.319992\n",
            "Epoch 2, Batch 260, Loss: 185.6343, Main: 1.3143, Load Balance: 184.319992\n",
            "Epoch 2, Batch 270, Loss: 185.6373, Main: 1.3173, Load Balance: 184.319992\n",
            "Epoch 2, Batch 280, Loss: 185.6564, Main: 1.3364, Load Balance: 184.319992\n",
            "Epoch 2, Batch 290, Loss: 185.5960, Main: 1.2760, Load Balance: 184.319992\n",
            "Epoch 2, Batch 300, Loss: 185.6049, Main: 1.2849, Load Balance: 184.319992\n",
            "Epoch 2, Batch 310, Loss: 185.6005, Main: 1.2805, Load Balance: 184.319992\n",
            "Epoch 2, Batch 320, Loss: 185.6248, Main: 1.3048, Load Balance: 184.319992\n",
            "Epoch 2, Batch 330, Loss: 185.6111, Main: 1.2911, Load Balance: 184.319992\n",
            "Epoch 2, Batch 340, Loss: 185.5939, Main: 1.2739, Load Balance: 184.319992\n",
            "Epoch 2, Batch 350, Loss: 185.6255, Main: 1.3055, Load Balance: 184.319992\n",
            "Epoch 2, Batch 360, Loss: 185.6543, Main: 1.3344, Load Balance: 184.319992\n",
            "Epoch 2, Batch 370, Loss: 185.6411, Main: 1.3211, Load Balance: 184.319992\n",
            "Epoch 2, Batch 380, Loss: 185.6030, Main: 1.2830, Load Balance: 184.319992\n",
            "Epoch 2, Batch 390, Loss: 185.5338, Main: 1.2138, Load Balance: 184.319992\n",
            "Epoch 2, Batch 400, Loss: 185.6120, Main: 1.2920, Load Balance: 184.319992\n",
            "Epoch 2, Batch 410, Loss: 185.6361, Main: 1.3161, Load Balance: 184.319992\n",
            "Epoch 2, Batch 420, Loss: 185.6071, Main: 1.2871, Load Balance: 184.319992\n",
            "Epoch 2, Batch 430, Loss: 185.5752, Main: 1.2552, Load Balance: 184.319992\n",
            "Epoch 2, Batch 440, Loss: 185.6056, Main: 1.2856, Load Balance: 184.319992\n",
            "Epoch 2, Batch 450, Loss: 185.6210, Main: 1.3010, Load Balance: 184.319992\n",
            "Epoch 2, Batch 460, Loss: 185.6144, Main: 1.2944, Load Balance: 184.319992\n",
            "Epoch 2, Batch 470, Loss: 185.5672, Main: 1.2472, Load Balance: 184.319992\n",
            "Epoch 2, Batch 480, Loss: 185.6332, Main: 1.3132, Load Balance: 184.319992\n",
            "Epoch 2, Batch 490, Loss: 185.6610, Main: 1.3410, Load Balance: 184.319992\n",
            "Epoch 2, Batch 500, Loss: 185.6546, Main: 1.3346, Load Balance: 184.319992\n",
            "Epoch 2, Batch 510, Loss: 185.6151, Main: 1.2951, Load Balance: 184.319992\n",
            "Epoch 2, Batch 520, Loss: 185.5911, Main: 1.2711, Load Balance: 184.319992\n",
            "Epoch 2, Batch 530, Loss: 185.6674, Main: 1.3474, Load Balance: 184.319992\n",
            "Epoch 2, Batch 540, Loss: 185.6017, Main: 1.2818, Load Balance: 184.319992\n",
            "Epoch 2, Batch 550, Loss: 185.5577, Main: 1.2377, Load Balance: 184.319992\n",
            "Epoch 2, Batch 560, Loss: 185.5899, Main: 1.2699, Load Balance: 184.319992\n",
            "Epoch 2, Batch 570, Loss: 185.6256, Main: 1.3056, Load Balance: 184.319992\n",
            "Epoch 2, Batch 580, Loss: 185.5359, Main: 1.2159, Load Balance: 184.319992\n",
            "Epoch 2, Batch 590, Loss: 185.5945, Main: 1.2745, Load Balance: 184.319992\n",
            "Epoch 2, Batch 600, Loss: 185.5412, Main: 1.2212, Load Balance: 184.319992\n",
            "Epoch 2 completed. Avg Loss: 1.2967, Avg LB Loss: 184.151015\n",
            "Epoch 3, Batch 0, Loss: 185.5083, Main: 1.1883, Load Balance: 184.319992\n",
            "Epoch 3, Batch 10, Loss: 185.5310, Main: 1.2110, Load Balance: 184.319992\n",
            "Epoch 3, Batch 20, Loss: 185.6172, Main: 1.2972, Load Balance: 184.319992\n",
            "Epoch 3, Batch 30, Loss: 185.5916, Main: 1.2716, Load Balance: 184.319992\n",
            "Epoch 3, Batch 40, Loss: 185.5667, Main: 1.2467, Load Balance: 184.319992\n",
            "Epoch 3, Batch 50, Loss: 185.5521, Main: 1.2321, Load Balance: 184.319992\n",
            "Epoch 3, Batch 60, Loss: 185.5443, Main: 1.2243, Load Balance: 184.319992\n",
            "Epoch 3, Batch 70, Loss: 185.6206, Main: 1.3006, Load Balance: 184.319992\n",
            "Epoch 3, Batch 80, Loss: 185.5452, Main: 1.2253, Load Balance: 184.319992\n",
            "Epoch 3, Batch 90, Loss: 185.5086, Main: 1.1886, Load Balance: 184.319992\n",
            "Epoch 3, Batch 100, Loss: 185.4906, Main: 1.1706, Load Balance: 184.319992\n",
            "Epoch 3, Batch 110, Loss: 185.4998, Main: 1.1798, Load Balance: 184.319992\n",
            "Epoch 3, Batch 120, Loss: 185.5650, Main: 1.2450, Load Balance: 184.319992\n",
            "Epoch 3, Batch 130, Loss: 185.4995, Main: 1.1795, Load Balance: 184.319992\n",
            "Epoch 3, Batch 140, Loss: 185.4949, Main: 1.1749, Load Balance: 184.319992\n",
            "Epoch 3, Batch 150, Loss: 185.4908, Main: 1.1708, Load Balance: 184.319992\n",
            "Epoch 3, Batch 160, Loss: 185.5031, Main: 1.1831, Load Balance: 184.319992\n",
            "Epoch 3, Batch 170, Loss: 185.5679, Main: 1.2479, Load Balance: 184.319992\n",
            "Epoch 3, Batch 180, Loss: 185.4729, Main: 1.1529, Load Balance: 184.319992\n",
            "Epoch 3, Batch 190, Loss: 185.5595, Main: 1.2395, Load Balance: 184.319992\n",
            "Epoch 3, Batch 200, Loss: 185.4256, Main: 1.1056, Load Balance: 184.319992\n",
            "Epoch 3, Batch 210, Loss: 185.5005, Main: 1.1805, Load Balance: 184.319992\n",
            "Epoch 3, Batch 220, Loss: 185.4987, Main: 1.1787, Load Balance: 184.319992\n",
            "Epoch 3, Batch 230, Loss: 185.4257, Main: 1.1057, Load Balance: 184.319992\n",
            "Epoch 3, Batch 240, Loss: 185.4073, Main: 1.0873, Load Balance: 184.319992\n",
            "Epoch 3, Batch 250, Loss: 185.4108, Main: 1.0908, Load Balance: 184.319992\n",
            "Epoch 3, Batch 260, Loss: 185.4149, Main: 1.0949, Load Balance: 184.319992\n",
            "Epoch 3, Batch 270, Loss: 185.4135, Main: 1.0935, Load Balance: 184.319992\n",
            "Epoch 3, Batch 280, Loss: 185.4219, Main: 1.1019, Load Balance: 184.319992\n",
            "Epoch 3, Batch 290, Loss: 185.4251, Main: 1.1051, Load Balance: 184.319992\n",
            "Epoch 3, Batch 300, Loss: 185.3898, Main: 1.0698, Load Balance: 184.319992\n",
            "Epoch 3, Batch 310, Loss: 185.4234, Main: 1.1034, Load Balance: 184.319992\n",
            "Epoch 3, Batch 320, Loss: 185.3911, Main: 1.0711, Load Balance: 184.319992\n",
            "Epoch 3, Batch 330, Loss: 185.3818, Main: 1.0618, Load Balance: 184.319992\n",
            "Epoch 3, Batch 340, Loss: 185.3997, Main: 1.0797, Load Balance: 184.319992\n",
            "Epoch 3, Batch 350, Loss: 185.3815, Main: 1.0615, Load Balance: 184.319992\n",
            "Epoch 3, Batch 360, Loss: 185.3945, Main: 1.0745, Load Balance: 184.319992\n",
            "Epoch 3, Batch 370, Loss: 185.3595, Main: 1.0395, Load Balance: 184.319992\n",
            "Epoch 3, Batch 380, Loss: 185.2695, Main: 0.9495, Load Balance: 184.319992\n",
            "Epoch 3, Batch 390, Loss: 185.3530, Main: 1.0330, Load Balance: 184.319992\n",
            "Epoch 3, Batch 400, Loss: 185.3974, Main: 1.0775, Load Balance: 184.319992\n",
            "Epoch 3, Batch 410, Loss: 185.2734, Main: 0.9534, Load Balance: 184.319992\n",
            "Epoch 3, Batch 420, Loss: 185.3507, Main: 1.0307, Load Balance: 184.319992\n",
            "Epoch 3, Batch 430, Loss: 185.3966, Main: 1.0766, Load Balance: 184.319992\n",
            "Epoch 3, Batch 440, Loss: 185.3966, Main: 1.0766, Load Balance: 184.319992\n",
            "Epoch 3, Batch 450, Loss: 185.4079, Main: 1.0879, Load Balance: 184.319992\n",
            "Epoch 3, Batch 460, Loss: 185.3743, Main: 1.0543, Load Balance: 184.319992\n",
            "Epoch 3, Batch 470, Loss: 185.3113, Main: 0.9913, Load Balance: 184.319992\n",
            "Epoch 3, Batch 480, Loss: 185.3110, Main: 0.9910, Load Balance: 184.319992\n",
            "Epoch 3, Batch 490, Loss: 185.3689, Main: 1.0489, Load Balance: 184.319992\n",
            "Epoch 3, Batch 500, Loss: 185.3230, Main: 1.0030, Load Balance: 184.319992\n",
            "Epoch 3, Batch 510, Loss: 185.2533, Main: 0.9333, Load Balance: 184.319992\n",
            "Epoch 3, Batch 520, Loss: 185.3939, Main: 1.0739, Load Balance: 184.319992\n",
            "Epoch 3, Batch 530, Loss: 185.3730, Main: 1.0530, Load Balance: 184.319992\n",
            "Epoch 3, Batch 540, Loss: 185.2651, Main: 0.9451, Load Balance: 184.319992\n",
            "Epoch 3, Batch 550, Loss: 185.3007, Main: 0.9807, Load Balance: 184.319992\n",
            "Epoch 3, Batch 560, Loss: 185.3104, Main: 0.9904, Load Balance: 184.319992\n",
            "Epoch 3, Batch 570, Loss: 185.3393, Main: 1.0193, Load Balance: 184.319992\n",
            "Epoch 3, Batch 580, Loss: 185.2897, Main: 0.9697, Load Balance: 184.319992\n",
            "Epoch 3, Batch 590, Loss: 185.2200, Main: 0.9000, Load Balance: 184.319992\n",
            "Epoch 3, Batch 600, Loss: 185.2289, Main: 0.9089, Load Balance: 184.319992\n",
            "Epoch 3 completed. Avg Loss: 1.1012, Avg LB Loss: 184.151015\n",
            "Epoch 4, Batch 0, Loss: 185.2634, Main: 0.9434, Load Balance: 184.319992\n",
            "Epoch 4, Batch 10, Loss: 185.2894, Main: 0.9694, Load Balance: 184.319992\n",
            "Epoch 4, Batch 20, Loss: 185.2871, Main: 0.9671, Load Balance: 184.319992\n",
            "Epoch 4, Batch 30, Loss: 185.2660, Main: 0.9460, Load Balance: 184.319992\n",
            "Epoch 4, Batch 40, Loss: 185.3085, Main: 0.9885, Load Balance: 184.319992\n",
            "Epoch 4, Batch 50, Loss: 185.2168, Main: 0.8968, Load Balance: 184.319992\n",
            "Epoch 4, Batch 60, Loss: 185.2677, Main: 0.9477, Load Balance: 184.319992\n",
            "Epoch 4, Batch 70, Loss: 185.2859, Main: 0.9659, Load Balance: 184.319992\n",
            "Epoch 4, Batch 80, Loss: 185.2242, Main: 0.9043, Load Balance: 184.319992\n",
            "Epoch 4, Batch 90, Loss: 185.2292, Main: 0.9092, Load Balance: 184.319992\n",
            "Epoch 4, Batch 100, Loss: 185.1964, Main: 0.8764, Load Balance: 184.319992\n",
            "Epoch 4, Batch 110, Loss: 185.1925, Main: 0.8725, Load Balance: 184.319992\n",
            "Epoch 4, Batch 120, Loss: 185.2550, Main: 0.9350, Load Balance: 184.319992\n",
            "Epoch 4, Batch 130, Loss: 185.2645, Main: 0.9445, Load Balance: 184.319992\n",
            "Epoch 4, Batch 140, Loss: 185.2906, Main: 0.9706, Load Balance: 184.319992\n",
            "Epoch 4, Batch 150, Loss: 185.2228, Main: 0.9028, Load Balance: 184.319992\n",
            "Epoch 4, Batch 160, Loss: 185.1581, Main: 0.8381, Load Balance: 184.319992\n",
            "Epoch 4, Batch 170, Loss: 185.3784, Main: 1.0584, Load Balance: 184.319992\n",
            "Epoch 4, Batch 180, Loss: 185.2433, Main: 0.9233, Load Balance: 184.319992\n",
            "Epoch 4, Batch 190, Loss: 185.2821, Main: 0.9621, Load Balance: 184.319992\n",
            "Epoch 4, Batch 200, Loss: 185.1455, Main: 0.8255, Load Balance: 184.319992\n",
            "Epoch 4, Batch 210, Loss: 185.1255, Main: 0.8055, Load Balance: 184.319992\n",
            "Epoch 4, Batch 220, Loss: 185.1180, Main: 0.7981, Load Balance: 184.319992\n",
            "Epoch 4, Batch 230, Loss: 185.2413, Main: 0.9213, Load Balance: 184.319992\n",
            "Epoch 4, Batch 240, Loss: 185.1813, Main: 0.8613, Load Balance: 184.319992\n",
            "Epoch 4, Batch 250, Loss: 185.2707, Main: 0.9507, Load Balance: 184.319992\n",
            "Epoch 4, Batch 260, Loss: 185.1788, Main: 0.8589, Load Balance: 184.319992\n",
            "Epoch 4, Batch 270, Loss: 185.2452, Main: 0.9252, Load Balance: 184.319992\n",
            "Epoch 4, Batch 280, Loss: 185.1529, Main: 0.8329, Load Balance: 184.319992\n",
            "Epoch 4, Batch 290, Loss: 185.1850, Main: 0.8650, Load Balance: 184.319992\n",
            "Epoch 4, Batch 300, Loss: 185.1413, Main: 0.8213, Load Balance: 184.319992\n",
            "Epoch 4, Batch 310, Loss: 185.2123, Main: 0.8923, Load Balance: 184.319992\n",
            "Epoch 4, Batch 320, Loss: 185.1743, Main: 0.8543, Load Balance: 184.319992\n",
            "Epoch 4, Batch 330, Loss: 185.2505, Main: 0.9305, Load Balance: 184.319992\n",
            "Epoch 4, Batch 340, Loss: 185.2402, Main: 0.9202, Load Balance: 184.319992\n",
            "Epoch 4, Batch 350, Loss: 185.1532, Main: 0.8332, Load Balance: 184.319992\n",
            "Epoch 4, Batch 360, Loss: 185.1757, Main: 0.8557, Load Balance: 184.319992\n",
            "Epoch 4, Batch 370, Loss: 185.1539, Main: 0.8339, Load Balance: 184.319992\n",
            "Epoch 4, Batch 380, Loss: 185.1967, Main: 0.8767, Load Balance: 184.319992\n",
            "Epoch 4, Batch 390, Loss: 185.1629, Main: 0.8429, Load Balance: 184.319992\n",
            "Epoch 4, Batch 400, Loss: 185.1947, Main: 0.8747, Load Balance: 184.319992\n",
            "Epoch 4, Batch 410, Loss: 185.1482, Main: 0.8282, Load Balance: 184.319992\n",
            "Epoch 4, Batch 420, Loss: 185.1240, Main: 0.8040, Load Balance: 184.319992\n",
            "Epoch 4, Batch 430, Loss: 185.0796, Main: 0.7596, Load Balance: 184.319992\n",
            "Epoch 4, Batch 440, Loss: 185.1616, Main: 0.8416, Load Balance: 184.319992\n",
            "Epoch 4, Batch 450, Loss: 185.0837, Main: 0.7637, Load Balance: 184.319992\n",
            "Epoch 4, Batch 460, Loss: 185.1329, Main: 0.8129, Load Balance: 184.319992\n",
            "Epoch 4, Batch 470, Loss: 185.1604, Main: 0.8404, Load Balance: 184.319992\n",
            "Epoch 4, Batch 480, Loss: 185.1757, Main: 0.8557, Load Balance: 184.319992\n",
            "Epoch 4, Batch 490, Loss: 185.1426, Main: 0.8226, Load Balance: 184.319992\n",
            "Epoch 4, Batch 500, Loss: 185.0584, Main: 0.7384, Load Balance: 184.319992\n",
            "Epoch 4, Batch 510, Loss: 185.1020, Main: 0.7820, Load Balance: 184.319992\n",
            "Epoch 4, Batch 520, Loss: 185.1330, Main: 0.8131, Load Balance: 184.319992\n",
            "Epoch 4, Batch 530, Loss: 185.0735, Main: 0.7535, Load Balance: 184.319992\n",
            "Epoch 4, Batch 540, Loss: 185.1118, Main: 0.7919, Load Balance: 184.319992\n",
            "Epoch 4, Batch 550, Loss: 185.0974, Main: 0.7774, Load Balance: 184.319992\n",
            "Epoch 4, Batch 560, Loss: 184.9818, Main: 0.6618, Load Balance: 184.319992\n",
            "Epoch 4, Batch 570, Loss: 185.1298, Main: 0.8098, Load Balance: 184.319992\n",
            "Epoch 4, Batch 580, Loss: 185.1548, Main: 0.8348, Load Balance: 184.319992\n",
            "Epoch 4, Batch 590, Loss: 185.0421, Main: 0.7221, Load Balance: 184.319992\n",
            "Epoch 4, Batch 600, Loss: 185.1062, Main: 0.7862, Load Balance: 184.319992\n",
            "Epoch 4 completed. Avg Loss: 0.8602, Avg LB Loss: 184.151015\n",
            "Epoch 5, Batch 0, Loss: 185.1119, Main: 0.7919, Load Balance: 184.319992\n",
            "Epoch 5, Batch 10, Loss: 185.1532, Main: 0.8332, Load Balance: 184.319992\n",
            "Epoch 5, Batch 20, Loss: 185.1207, Main: 0.8007, Load Balance: 184.319992\n",
            "Epoch 5, Batch 30, Loss: 185.0816, Main: 0.7617, Load Balance: 184.319992\n",
            "Epoch 5, Batch 40, Loss: 185.1781, Main: 0.8581, Load Balance: 184.319992\n",
            "Epoch 5, Batch 50, Loss: 185.1086, Main: 0.7886, Load Balance: 184.319992\n",
            "Epoch 5, Batch 60, Loss: 185.1557, Main: 0.8357, Load Balance: 184.319992\n",
            "Epoch 5, Batch 70, Loss: 185.1130, Main: 0.7930, Load Balance: 184.319992\n",
            "Epoch 5, Batch 80, Loss: 185.0812, Main: 0.7612, Load Balance: 184.319992\n",
            "Epoch 5, Batch 90, Loss: 185.0310, Main: 0.7110, Load Balance: 184.319992\n",
            "Epoch 5, Batch 100, Loss: 185.1195, Main: 0.7995, Load Balance: 184.319992\n",
            "Epoch 5, Batch 110, Loss: 185.1600, Main: 0.8401, Load Balance: 184.319992\n",
            "Epoch 5, Batch 120, Loss: 185.1223, Main: 0.8023, Load Balance: 184.319992\n",
            "Epoch 5, Batch 130, Loss: 185.1244, Main: 0.8044, Load Balance: 184.319992\n",
            "Epoch 5, Batch 140, Loss: 185.0444, Main: 0.7244, Load Balance: 184.319992\n",
            "Epoch 5, Batch 150, Loss: 185.1185, Main: 0.7985, Load Balance: 184.319992\n",
            "Epoch 5, Batch 160, Loss: 185.0728, Main: 0.7528, Load Balance: 184.319992\n",
            "Epoch 5, Batch 170, Loss: 185.0524, Main: 0.7324, Load Balance: 184.319992\n",
            "Epoch 5, Batch 180, Loss: 185.0155, Main: 0.6955, Load Balance: 184.319992\n",
            "Epoch 5, Batch 190, Loss: 185.0412, Main: 0.7212, Load Balance: 184.319992\n",
            "Epoch 5, Batch 200, Loss: 185.0234, Main: 0.7034, Load Balance: 184.319992\n",
            "Epoch 5, Batch 210, Loss: 185.1245, Main: 0.8045, Load Balance: 184.319992\n",
            "Epoch 5, Batch 220, Loss: 185.0273, Main: 0.7073, Load Balance: 184.319992\n",
            "Epoch 5, Batch 230, Loss: 185.0092, Main: 0.6892, Load Balance: 184.319992\n",
            "Epoch 5, Batch 240, Loss: 185.0060, Main: 0.6860, Load Balance: 184.319992\n",
            "Epoch 5, Batch 250, Loss: 185.0639, Main: 0.7439, Load Balance: 184.319992\n",
            "Epoch 5, Batch 260, Loss: 185.0346, Main: 0.7146, Load Balance: 184.319992\n",
            "Epoch 5, Batch 270, Loss: 185.0571, Main: 0.7371, Load Balance: 184.319992\n",
            "Epoch 5, Batch 280, Loss: 185.0232, Main: 0.7032, Load Balance: 184.319992\n",
            "Epoch 5, Batch 290, Loss: 185.1041, Main: 0.7841, Load Balance: 184.319992\n",
            "Epoch 5, Batch 300, Loss: 185.0474, Main: 0.7274, Load Balance: 184.319992\n",
            "Epoch 5, Batch 310, Loss: 185.0535, Main: 0.7335, Load Balance: 184.319992\n",
            "Epoch 5, Batch 320, Loss: 185.0798, Main: 0.7599, Load Balance: 184.319992\n",
            "Epoch 5, Batch 330, Loss: 184.9810, Main: 0.6610, Load Balance: 184.319992\n",
            "Epoch 5, Batch 340, Loss: 184.9654, Main: 0.6454, Load Balance: 184.319992\n",
            "Epoch 5, Batch 350, Loss: 185.1183, Main: 0.7983, Load Balance: 184.319992\n",
            "Epoch 5, Batch 360, Loss: 185.0346, Main: 0.7146, Load Balance: 184.319992\n",
            "Epoch 5, Batch 370, Loss: 185.0145, Main: 0.6946, Load Balance: 184.319992\n",
            "Epoch 5, Batch 380, Loss: 185.0662, Main: 0.7462, Load Balance: 184.319992\n",
            "Epoch 5, Batch 390, Loss: 185.0465, Main: 0.7265, Load Balance: 184.319992\n",
            "Epoch 5, Batch 400, Loss: 185.0110, Main: 0.6910, Load Balance: 184.319992\n",
            "Epoch 5, Batch 410, Loss: 184.9905, Main: 0.6705, Load Balance: 184.319992\n",
            "Epoch 5, Batch 420, Loss: 184.9607, Main: 0.6407, Load Balance: 184.319992\n",
            "Epoch 5, Batch 430, Loss: 185.0737, Main: 0.7537, Load Balance: 184.319992\n",
            "Epoch 5, Batch 440, Loss: 185.0758, Main: 0.7558, Load Balance: 184.319992\n",
            "Epoch 5, Batch 450, Loss: 185.0065, Main: 0.6866, Load Balance: 184.319992\n",
            "Epoch 5, Batch 460, Loss: 185.0151, Main: 0.6951, Load Balance: 184.319992\n",
            "Epoch 5, Batch 470, Loss: 185.0501, Main: 0.7301, Load Balance: 184.319992\n",
            "Epoch 5, Batch 480, Loss: 184.9951, Main: 0.6752, Load Balance: 184.319992\n",
            "Epoch 5, Batch 490, Loss: 184.9828, Main: 0.6628, Load Balance: 184.319992\n",
            "Epoch 5, Batch 500, Loss: 185.0642, Main: 0.7442, Load Balance: 184.319992\n",
            "Epoch 5, Batch 510, Loss: 184.9918, Main: 0.6718, Load Balance: 184.319992\n",
            "Epoch 5, Batch 520, Loss: 185.0089, Main: 0.6890, Load Balance: 184.319992\n",
            "Epoch 5, Batch 530, Loss: 185.0574, Main: 0.7374, Load Balance: 184.319992\n",
            "Epoch 5, Batch 540, Loss: 185.0362, Main: 0.7162, Load Balance: 184.319992\n",
            "Epoch 5, Batch 550, Loss: 184.9463, Main: 0.6263, Load Balance: 184.319992\n",
            "Epoch 5, Batch 560, Loss: 184.9827, Main: 0.6627, Load Balance: 184.319992\n",
            "Epoch 5, Batch 570, Loss: 185.0393, Main: 0.7193, Load Balance: 184.319992\n",
            "Epoch 5, Batch 580, Loss: 184.9827, Main: 0.6627, Load Balance: 184.319992\n",
            "Epoch 5, Batch 590, Loss: 184.9755, Main: 0.6555, Load Balance: 184.319992\n",
            "Epoch 5, Batch 600, Loss: 184.9440, Main: 0.6240, Load Balance: 184.319992\n",
            "Epoch 5 completed. Avg Loss: 0.7213, Avg LB Loss: 184.151015\n",
            "Epoch 6, Batch 0, Loss: 184.9206, Main: 0.6006, Load Balance: 184.319992\n",
            "Epoch 6, Batch 10, Loss: 184.9700, Main: 0.6500, Load Balance: 184.319992\n",
            "Epoch 6, Batch 20, Loss: 184.9702, Main: 0.6502, Load Balance: 184.319992\n",
            "Epoch 6, Batch 30, Loss: 184.9538, Main: 0.6338, Load Balance: 184.319992\n",
            "Epoch 6, Batch 40, Loss: 185.0262, Main: 0.7062, Load Balance: 184.319992\n",
            "Epoch 6, Batch 50, Loss: 184.9616, Main: 0.6417, Load Balance: 184.319992\n",
            "Epoch 6, Batch 60, Loss: 184.9836, Main: 0.6636, Load Balance: 184.319992\n",
            "Epoch 6, Batch 70, Loss: 184.9731, Main: 0.6531, Load Balance: 184.319992\n",
            "Epoch 6, Batch 80, Loss: 184.9402, Main: 0.6202, Load Balance: 184.319992\n",
            "Epoch 6, Batch 90, Loss: 184.9969, Main: 0.6769, Load Balance: 184.319992\n",
            "Epoch 6, Batch 100, Loss: 184.9311, Main: 0.6111, Load Balance: 184.319992\n",
            "Epoch 6, Batch 110, Loss: 184.9275, Main: 0.6075, Load Balance: 184.319992\n",
            "Epoch 6, Batch 120, Loss: 184.9744, Main: 0.6544, Load Balance: 184.319992\n",
            "Epoch 6, Batch 130, Loss: 184.9914, Main: 0.6714, Load Balance: 184.319992\n",
            "Epoch 6, Batch 140, Loss: 184.9893, Main: 0.6693, Load Balance: 184.319992\n",
            "Epoch 6, Batch 150, Loss: 184.9595, Main: 0.6395, Load Balance: 184.319992\n",
            "Epoch 6, Batch 160, Loss: 184.9892, Main: 0.6692, Load Balance: 184.319992\n",
            "Epoch 6, Batch 170, Loss: 185.0126, Main: 0.6926, Load Balance: 184.319992\n",
            "Epoch 6, Batch 180, Loss: 184.9251, Main: 0.6051, Load Balance: 184.319992\n",
            "Epoch 6, Batch 190, Loss: 184.9278, Main: 0.6078, Load Balance: 184.319992\n",
            "Epoch 6, Batch 200, Loss: 185.0107, Main: 0.6907, Load Balance: 184.319992\n",
            "Epoch 6, Batch 210, Loss: 184.9176, Main: 0.5976, Load Balance: 184.319992\n",
            "Epoch 6, Batch 220, Loss: 184.9495, Main: 0.6295, Load Balance: 184.319992\n",
            "Epoch 6, Batch 230, Loss: 185.0126, Main: 0.6926, Load Balance: 184.319992\n",
            "Epoch 6, Batch 240, Loss: 184.8766, Main: 0.5566, Load Balance: 184.319992\n",
            "Epoch 6, Batch 250, Loss: 184.9493, Main: 0.6293, Load Balance: 184.319992\n",
            "Epoch 6, Batch 260, Loss: 184.8152, Main: 0.4952, Load Balance: 184.319992\n",
            "Epoch 6, Batch 270, Loss: 184.9920, Main: 0.6720, Load Balance: 184.319992\n",
            "Epoch 6, Batch 280, Loss: 184.9467, Main: 0.6268, Load Balance: 184.319992\n",
            "Epoch 6, Batch 290, Loss: 185.0176, Main: 0.6976, Load Balance: 184.319992\n",
            "Epoch 6, Batch 300, Loss: 184.8849, Main: 0.5649, Load Balance: 184.319992\n",
            "Epoch 6, Batch 310, Loss: 184.8876, Main: 0.5676, Load Balance: 184.319992\n",
            "Epoch 6, Batch 320, Loss: 184.9047, Main: 0.5847, Load Balance: 184.319992\n",
            "Epoch 6, Batch 330, Loss: 184.8786, Main: 0.5586, Load Balance: 184.319992\n",
            "Epoch 6, Batch 340, Loss: 184.9040, Main: 0.5840, Load Balance: 184.319992\n",
            "Epoch 6, Batch 350, Loss: 184.9987, Main: 0.6787, Load Balance: 184.319992\n",
            "Epoch 6, Batch 360, Loss: 185.0376, Main: 0.7176, Load Balance: 184.319992\n",
            "Epoch 6, Batch 370, Loss: 184.9404, Main: 0.6204, Load Balance: 184.319992\n",
            "Epoch 6, Batch 380, Loss: 184.9038, Main: 0.5838, Load Balance: 184.319992\n",
            "Epoch 6, Batch 390, Loss: 184.9647, Main: 0.6447, Load Balance: 184.319992\n",
            "Epoch 6, Batch 400, Loss: 184.8675, Main: 0.5475, Load Balance: 184.319992\n",
            "Epoch 6, Batch 410, Loss: 184.9346, Main: 0.6146, Load Balance: 184.319992\n",
            "Epoch 6, Batch 420, Loss: 184.8428, Main: 0.5229, Load Balance: 184.319992\n",
            "Epoch 6, Batch 430, Loss: 184.8959, Main: 0.5759, Load Balance: 184.319992\n",
            "Epoch 6, Batch 440, Loss: 184.9653, Main: 0.6453, Load Balance: 184.319992\n",
            "Epoch 6, Batch 450, Loss: 184.9047, Main: 0.5847, Load Balance: 184.319992\n",
            "Epoch 6, Batch 460, Loss: 184.8873, Main: 0.5673, Load Balance: 184.319992\n",
            "Epoch 6, Batch 470, Loss: 184.9482, Main: 0.6282, Load Balance: 184.319992\n",
            "Epoch 6, Batch 480, Loss: 184.8445, Main: 0.5245, Load Balance: 184.319992\n",
            "Epoch 6, Batch 490, Loss: 184.8227, Main: 0.5027, Load Balance: 184.319992\n",
            "Epoch 6, Batch 500, Loss: 184.8895, Main: 0.5695, Load Balance: 184.319992\n",
            "Epoch 6, Batch 510, Loss: 184.8994, Main: 0.5794, Load Balance: 184.319992\n",
            "Epoch 6, Batch 520, Loss: 184.8957, Main: 0.5757, Load Balance: 184.319992\n",
            "Epoch 6, Batch 530, Loss: 184.8462, Main: 0.5262, Load Balance: 184.319992\n",
            "Epoch 6, Batch 540, Loss: 184.8664, Main: 0.5464, Load Balance: 184.319992\n",
            "Epoch 6, Batch 550, Loss: 184.8849, Main: 0.5649, Load Balance: 184.319992\n",
            "Epoch 6, Batch 560, Loss: 184.9205, Main: 0.6005, Load Balance: 184.319992\n",
            "Epoch 6, Batch 570, Loss: 184.9088, Main: 0.5888, Load Balance: 184.319992\n",
            "Epoch 6, Batch 580, Loss: 184.9053, Main: 0.5853, Load Balance: 184.319992\n",
            "Epoch 6, Batch 590, Loss: 184.8948, Main: 0.5748, Load Balance: 184.319992\n",
            "Epoch 6, Batch 600, Loss: 184.9155, Main: 0.5955, Load Balance: 184.319992\n",
            "Epoch 6 completed. Avg Loss: 0.6203, Avg LB Loss: 184.151015\n",
            "Epoch 7, Batch 0, Loss: 184.8757, Main: 0.5557, Load Balance: 184.319992\n",
            "Epoch 7, Batch 10, Loss: 184.8755, Main: 0.5555, Load Balance: 184.319992\n",
            "Epoch 7, Batch 20, Loss: 184.8164, Main: 0.4964, Load Balance: 184.319992\n",
            "Epoch 7, Batch 30, Loss: 184.8121, Main: 0.4921, Load Balance: 184.319992\n",
            "Epoch 7, Batch 40, Loss: 184.8904, Main: 0.5704, Load Balance: 184.319992\n",
            "Epoch 7, Batch 50, Loss: 184.9621, Main: 0.6421, Load Balance: 184.319992\n",
            "Epoch 7, Batch 60, Loss: 184.9554, Main: 0.6354, Load Balance: 184.319992\n",
            "Epoch 7, Batch 70, Loss: 184.9146, Main: 0.5946, Load Balance: 184.319992\n",
            "Epoch 7, Batch 80, Loss: 184.8624, Main: 0.5425, Load Balance: 184.319992\n",
            "Epoch 7, Batch 90, Loss: 184.8483, Main: 0.5283, Load Balance: 184.319992\n",
            "Epoch 7, Batch 100, Loss: 184.8828, Main: 0.5628, Load Balance: 184.319992\n",
            "Epoch 7, Batch 110, Loss: 184.8225, Main: 0.5025, Load Balance: 184.319992\n",
            "Epoch 7, Batch 120, Loss: 184.9865, Main: 0.6665, Load Balance: 184.319992\n",
            "Epoch 7, Batch 130, Loss: 184.9451, Main: 0.6251, Load Balance: 184.319992\n",
            "Epoch 7, Batch 140, Loss: 184.8889, Main: 0.5690, Load Balance: 184.319992\n",
            "Epoch 7, Batch 150, Loss: 184.8622, Main: 0.5423, Load Balance: 184.319992\n",
            "Epoch 7, Batch 160, Loss: 184.8760, Main: 0.5560, Load Balance: 184.319992\n",
            "Epoch 7, Batch 170, Loss: 184.8776, Main: 0.5576, Load Balance: 184.319992\n",
            "Epoch 7, Batch 180, Loss: 184.8544, Main: 0.5344, Load Balance: 184.319992\n",
            "Epoch 7, Batch 190, Loss: 184.7883, Main: 0.4683, Load Balance: 184.319992\n",
            "Epoch 7, Batch 200, Loss: 184.7851, Main: 0.4652, Load Balance: 184.319992\n",
            "Epoch 7, Batch 210, Loss: 184.8055, Main: 0.4855, Load Balance: 184.319992\n",
            "Epoch 7, Batch 220, Loss: 184.9493, Main: 0.6293, Load Balance: 184.319992\n",
            "Epoch 7, Batch 230, Loss: 184.7711, Main: 0.4511, Load Balance: 184.319992\n",
            "Epoch 7, Batch 240, Loss: 184.8594, Main: 0.5394, Load Balance: 184.319992\n",
            "Epoch 7, Batch 250, Loss: 184.7730, Main: 0.4530, Load Balance: 184.319992\n",
            "Epoch 7, Batch 260, Loss: 184.8662, Main: 0.5462, Load Balance: 184.319992\n",
            "Epoch 7, Batch 270, Loss: 184.8708, Main: 0.5508, Load Balance: 184.319992\n",
            "Epoch 7, Batch 280, Loss: 184.8458, Main: 0.5258, Load Balance: 184.319992\n",
            "Epoch 7, Batch 290, Loss: 184.8498, Main: 0.5298, Load Balance: 184.319992\n",
            "Epoch 7, Batch 300, Loss: 184.8448, Main: 0.5248, Load Balance: 184.319992\n",
            "Epoch 7, Batch 310, Loss: 184.8889, Main: 0.5689, Load Balance: 184.319992\n",
            "Epoch 7, Batch 320, Loss: 184.8810, Main: 0.5610, Load Balance: 184.319992\n",
            "Epoch 7, Batch 330, Loss: 184.8927, Main: 0.5728, Load Balance: 184.319992\n",
            "Epoch 7, Batch 340, Loss: 184.7883, Main: 0.4683, Load Balance: 184.319992\n",
            "Epoch 7, Batch 350, Loss: 184.8111, Main: 0.4911, Load Balance: 184.319992\n",
            "Epoch 7, Batch 360, Loss: 184.8327, Main: 0.5127, Load Balance: 184.319992\n",
            "Epoch 7, Batch 370, Loss: 184.8745, Main: 0.5545, Load Balance: 184.319992\n",
            "Epoch 7, Batch 380, Loss: 184.8175, Main: 0.4975, Load Balance: 184.319992\n",
            "Epoch 7, Batch 390, Loss: 184.7912, Main: 0.4713, Load Balance: 184.319992\n",
            "Epoch 7, Batch 400, Loss: 184.8175, Main: 0.4975, Load Balance: 184.319992\n",
            "Epoch 7, Batch 410, Loss: 184.8940, Main: 0.5740, Load Balance: 184.319992\n",
            "Epoch 7, Batch 420, Loss: 184.8700, Main: 0.5500, Load Balance: 184.319992\n",
            "Epoch 7, Batch 430, Loss: 184.8215, Main: 0.5015, Load Balance: 184.319992\n",
            "Epoch 7, Batch 440, Loss: 184.8321, Main: 0.5121, Load Balance: 184.319992\n",
            "Epoch 7, Batch 450, Loss: 184.8019, Main: 0.4819, Load Balance: 184.319992\n",
            "Epoch 7, Batch 460, Loss: 184.8394, Main: 0.5194, Load Balance: 184.319992\n",
            "Epoch 7, Batch 470, Loss: 184.8914, Main: 0.5714, Load Balance: 184.319992\n",
            "Epoch 7, Batch 480, Loss: 184.8396, Main: 0.5196, Load Balance: 184.319992\n",
            "Epoch 7, Batch 490, Loss: 184.8819, Main: 0.5620, Load Balance: 184.319992\n",
            "Epoch 7, Batch 500, Loss: 184.8854, Main: 0.5654, Load Balance: 184.319992\n",
            "Epoch 7, Batch 510, Loss: 184.8845, Main: 0.5645, Load Balance: 184.319992\n",
            "Epoch 7, Batch 520, Loss: 184.8435, Main: 0.5235, Load Balance: 184.319992\n",
            "Epoch 7, Batch 530, Loss: 184.8257, Main: 0.5057, Load Balance: 184.319992\n",
            "Epoch 7, Batch 540, Loss: 184.7661, Main: 0.4461, Load Balance: 184.319992\n",
            "Epoch 7, Batch 550, Loss: 184.8503, Main: 0.5303, Load Balance: 184.319992\n",
            "Epoch 7, Batch 560, Loss: 184.7581, Main: 0.4381, Load Balance: 184.319992\n",
            "Epoch 7, Batch 570, Loss: 184.8003, Main: 0.4804, Load Balance: 184.319992\n",
            "Epoch 7, Batch 580, Loss: 184.8160, Main: 0.4960, Load Balance: 184.319992\n",
            "Epoch 7, Batch 590, Loss: 184.8408, Main: 0.5208, Load Balance: 184.319992\n",
            "Epoch 7, Batch 600, Loss: 184.9600, Main: 0.6400, Load Balance: 184.319992\n",
            "Epoch 7 completed. Avg Loss: 0.5329, Avg LB Loss: 184.151015\n",
            "Epoch 8, Batch 0, Loss: 184.8514, Main: 0.5314, Load Balance: 184.319992\n",
            "Epoch 8, Batch 10, Loss: 184.9079, Main: 0.5880, Load Balance: 184.319992\n",
            "Epoch 8, Batch 20, Loss: 184.8368, Main: 0.5168, Load Balance: 184.319992\n",
            "Epoch 8, Batch 30, Loss: 184.8297, Main: 0.5097, Load Balance: 184.319992\n",
            "Epoch 8, Batch 40, Loss: 184.8449, Main: 0.5249, Load Balance: 184.319992\n",
            "Epoch 8, Batch 50, Loss: 184.8343, Main: 0.5143, Load Balance: 184.319992\n",
            "Epoch 8, Batch 60, Loss: 184.8623, Main: 0.5423, Load Balance: 184.319992\n",
            "Epoch 8, Batch 70, Loss: 184.7976, Main: 0.4776, Load Balance: 184.319992\n",
            "Epoch 8, Batch 80, Loss: 184.8181, Main: 0.4981, Load Balance: 184.319992\n",
            "Epoch 8, Batch 90, Loss: 184.8236, Main: 0.5036, Load Balance: 184.319992\n",
            "Epoch 8, Batch 100, Loss: 184.8267, Main: 0.5067, Load Balance: 184.319992\n",
            "Epoch 8, Batch 110, Loss: 184.8487, Main: 0.5287, Load Balance: 184.319992\n",
            "Epoch 8, Batch 120, Loss: 184.7815, Main: 0.4616, Load Balance: 184.319992\n",
            "Epoch 8, Batch 130, Loss: 184.8198, Main: 0.4998, Load Balance: 184.319992\n",
            "Epoch 8, Batch 140, Loss: 184.7909, Main: 0.4709, Load Balance: 184.319992\n",
            "Epoch 8, Batch 150, Loss: 184.8504, Main: 0.5304, Load Balance: 184.319992\n",
            "Epoch 8, Batch 160, Loss: 184.8746, Main: 0.5546, Load Balance: 184.319992\n",
            "Epoch 8, Batch 170, Loss: 184.7299, Main: 0.4099, Load Balance: 184.319992\n",
            "Epoch 8, Batch 180, Loss: 184.8076, Main: 0.4876, Load Balance: 184.319992\n",
            "Epoch 8, Batch 190, Loss: 184.7312, Main: 0.4112, Load Balance: 184.319992\n",
            "Epoch 8, Batch 200, Loss: 184.8554, Main: 0.5354, Load Balance: 184.319992\n",
            "Epoch 8, Batch 210, Loss: 184.7986, Main: 0.4786, Load Balance: 184.319992\n",
            "Epoch 8, Batch 220, Loss: 184.9106, Main: 0.5906, Load Balance: 184.319992\n",
            "Epoch 8, Batch 230, Loss: 184.8626, Main: 0.5427, Load Balance: 184.319992\n",
            "Epoch 8, Batch 240, Loss: 184.8114, Main: 0.4914, Load Balance: 184.319992\n",
            "Epoch 8, Batch 250, Loss: 184.8332, Main: 0.5132, Load Balance: 184.319992\n",
            "Epoch 8, Batch 260, Loss: 184.7828, Main: 0.4628, Load Balance: 184.319992\n",
            "Epoch 8, Batch 270, Loss: 184.8124, Main: 0.4924, Load Balance: 184.319992\n",
            "Epoch 8, Batch 280, Loss: 184.8356, Main: 0.5156, Load Balance: 184.319992\n",
            "Epoch 8, Batch 290, Loss: 184.7564, Main: 0.4364, Load Balance: 184.319992\n",
            "Epoch 8, Batch 300, Loss: 184.7775, Main: 0.4575, Load Balance: 184.319992\n",
            "Epoch 8, Batch 310, Loss: 184.8261, Main: 0.5062, Load Balance: 184.319992\n",
            "Epoch 8, Batch 320, Loss: 184.8154, Main: 0.4954, Load Balance: 184.319992\n",
            "Epoch 8, Batch 330, Loss: 184.7659, Main: 0.4459, Load Balance: 184.319992\n",
            "Epoch 8, Batch 340, Loss: 184.7349, Main: 0.4149, Load Balance: 184.319992\n",
            "Epoch 8, Batch 350, Loss: 184.8139, Main: 0.4939, Load Balance: 184.319992\n",
            "Epoch 8, Batch 360, Loss: 184.8068, Main: 0.4868, Load Balance: 184.319992\n",
            "Epoch 8, Batch 370, Loss: 184.6894, Main: 0.3694, Load Balance: 184.319992\n",
            "Epoch 8, Batch 380, Loss: 184.7590, Main: 0.4390, Load Balance: 184.319992\n",
            "Epoch 8, Batch 390, Loss: 184.8607, Main: 0.5407, Load Balance: 184.319992\n",
            "Epoch 8, Batch 400, Loss: 184.7784, Main: 0.4584, Load Balance: 184.319992\n",
            "Epoch 8, Batch 410, Loss: 184.7250, Main: 0.4050, Load Balance: 184.319992\n",
            "Epoch 8, Batch 420, Loss: 184.7906, Main: 0.4706, Load Balance: 184.319992\n",
            "Epoch 8, Batch 430, Loss: 184.7913, Main: 0.4713, Load Balance: 184.319992\n",
            "Epoch 8, Batch 440, Loss: 184.8212, Main: 0.5012, Load Balance: 184.319992\n",
            "Epoch 8, Batch 450, Loss: 184.8252, Main: 0.5052, Load Balance: 184.319992\n",
            "Epoch 8, Batch 460, Loss: 184.7748, Main: 0.4548, Load Balance: 184.319992\n",
            "Epoch 8, Batch 470, Loss: 184.7589, Main: 0.4389, Load Balance: 184.319992\n",
            "Epoch 8, Batch 480, Loss: 184.8018, Main: 0.4819, Load Balance: 184.319992\n",
            "Epoch 8, Batch 490, Loss: 184.7149, Main: 0.3949, Load Balance: 184.319992\n",
            "Epoch 8, Batch 500, Loss: 184.7223, Main: 0.4024, Load Balance: 184.319992\n",
            "Epoch 8, Batch 510, Loss: 184.7883, Main: 0.4683, Load Balance: 184.319992\n",
            "Epoch 8, Batch 520, Loss: 184.7009, Main: 0.3809, Load Balance: 184.319992\n",
            "Epoch 8, Batch 530, Loss: 184.8039, Main: 0.4839, Load Balance: 184.319992\n",
            "Epoch 8, Batch 540, Loss: 184.8271, Main: 0.5071, Load Balance: 184.319992\n",
            "Epoch 8, Batch 550, Loss: 184.8827, Main: 0.5627, Load Balance: 184.319992\n",
            "Epoch 8, Batch 560, Loss: 184.7906, Main: 0.4706, Load Balance: 184.319992\n",
            "Epoch 8, Batch 570, Loss: 184.7883, Main: 0.4683, Load Balance: 184.319992\n",
            "Epoch 8, Batch 580, Loss: 184.8174, Main: 0.4975, Load Balance: 184.319992\n",
            "Epoch 8, Batch 590, Loss: 184.7578, Main: 0.4378, Load Balance: 184.319992\n",
            "Epoch 8, Batch 600, Loss: 184.8455, Main: 0.5255, Load Balance: 184.319992\n",
            "Epoch 8 completed. Avg Loss: 0.4843, Avg LB Loss: 184.151015\n",
            "Epoch 9, Batch 0, Loss: 184.7685, Main: 0.4485, Load Balance: 184.319992\n",
            "Epoch 9, Batch 10, Loss: 184.7539, Main: 0.4339, Load Balance: 184.319992\n",
            "Epoch 9, Batch 20, Loss: 184.7539, Main: 0.4339, Load Balance: 184.319992\n",
            "Epoch 9, Batch 30, Loss: 184.8029, Main: 0.4830, Load Balance: 184.319992\n",
            "Epoch 9, Batch 40, Loss: 184.7406, Main: 0.4206, Load Balance: 184.319992\n",
            "Epoch 9, Batch 50, Loss: 184.7201, Main: 0.4001, Load Balance: 184.319992\n",
            "Epoch 9, Batch 60, Loss: 184.7154, Main: 0.3954, Load Balance: 184.319992\n",
            "Epoch 9, Batch 70, Loss: 184.7684, Main: 0.4484, Load Balance: 184.319992\n",
            "Epoch 9, Batch 80, Loss: 184.8172, Main: 0.4972, Load Balance: 184.319992\n",
            "Epoch 9, Batch 90, Loss: 184.7792, Main: 0.4593, Load Balance: 184.319992\n",
            "Epoch 9, Batch 100, Loss: 184.7572, Main: 0.4373, Load Balance: 184.319992\n",
            "Epoch 9, Batch 110, Loss: 184.6976, Main: 0.3776, Load Balance: 184.319992\n",
            "Epoch 9, Batch 120, Loss: 184.6683, Main: 0.3483, Load Balance: 184.319992\n",
            "Epoch 9, Batch 130, Loss: 184.7301, Main: 0.4101, Load Balance: 184.319992\n",
            "Epoch 9, Batch 140, Loss: 184.6771, Main: 0.3571, Load Balance: 184.319992\n",
            "Epoch 9, Batch 150, Loss: 184.7245, Main: 0.4045, Load Balance: 184.319992\n",
            "Epoch 9, Batch 160, Loss: 184.6512, Main: 0.3312, Load Balance: 184.319992\n",
            "Epoch 9, Batch 170, Loss: 184.7897, Main: 0.4697, Load Balance: 184.319992\n",
            "Epoch 9, Batch 180, Loss: 184.7419, Main: 0.4219, Load Balance: 184.319992\n",
            "Epoch 9, Batch 190, Loss: 184.7733, Main: 0.4533, Load Balance: 184.319992\n",
            "Epoch 9, Batch 200, Loss: 184.6856, Main: 0.3656, Load Balance: 184.319992\n",
            "Epoch 9, Batch 210, Loss: 184.7082, Main: 0.3882, Load Balance: 184.319992\n",
            "Epoch 9, Batch 220, Loss: 184.7207, Main: 0.4007, Load Balance: 184.319992\n",
            "Epoch 9, Batch 230, Loss: 184.7816, Main: 0.4616, Load Balance: 184.319992\n",
            "Epoch 9, Batch 240, Loss: 184.9028, Main: 0.5828, Load Balance: 184.319992\n",
            "Epoch 9, Batch 250, Loss: 184.8259, Main: 0.5059, Load Balance: 184.319992\n",
            "Epoch 9, Batch 260, Loss: 184.8331, Main: 0.5131, Load Balance: 184.319992\n",
            "Epoch 9, Batch 270, Loss: 184.9707, Main: 0.6507, Load Balance: 184.319992\n",
            "Epoch 9, Batch 280, Loss: 184.8283, Main: 0.5083, Load Balance: 184.319992\n",
            "Epoch 9, Batch 290, Loss: 184.8429, Main: 0.5229, Load Balance: 184.319992\n",
            "Epoch 9, Batch 300, Loss: 184.7254, Main: 0.4054, Load Balance: 184.319992\n",
            "Epoch 9, Batch 310, Loss: 184.8111, Main: 0.4911, Load Balance: 184.319992\n",
            "Epoch 9, Batch 320, Loss: 184.7861, Main: 0.4662, Load Balance: 184.319992\n",
            "Epoch 9, Batch 330, Loss: 184.7536, Main: 0.4336, Load Balance: 184.319992\n",
            "Epoch 9, Batch 340, Loss: 184.7656, Main: 0.4456, Load Balance: 184.319992\n",
            "Epoch 9, Batch 350, Loss: 184.7605, Main: 0.4405, Load Balance: 184.319992\n",
            "Epoch 9, Batch 360, Loss: 184.7396, Main: 0.4196, Load Balance: 184.319992\n",
            "Epoch 9, Batch 370, Loss: 184.7527, Main: 0.4327, Load Balance: 184.319992\n",
            "Epoch 9, Batch 380, Loss: 184.7004, Main: 0.3804, Load Balance: 184.319992\n",
            "Epoch 9, Batch 390, Loss: 184.6918, Main: 0.3718, Load Balance: 184.319992\n",
            "Epoch 9, Batch 400, Loss: 184.7130, Main: 0.3930, Load Balance: 184.319992\n",
            "Epoch 9, Batch 410, Loss: 184.7781, Main: 0.4581, Load Balance: 184.319992\n",
            "Epoch 9, Batch 420, Loss: 184.7686, Main: 0.4486, Load Balance: 184.319992\n",
            "Epoch 9, Batch 430, Loss: 184.8204, Main: 0.5004, Load Balance: 184.319992\n",
            "Epoch 9, Batch 440, Loss: 184.7729, Main: 0.4529, Load Balance: 184.319992\n",
            "Epoch 9, Batch 450, Loss: 184.6700, Main: 0.3500, Load Balance: 184.319992\n",
            "Epoch 9, Batch 460, Loss: 184.6786, Main: 0.3586, Load Balance: 184.319992\n",
            "Epoch 9, Batch 470, Loss: 184.7551, Main: 0.4351, Load Balance: 184.319992\n",
            "Epoch 9, Batch 480, Loss: 184.7612, Main: 0.4413, Load Balance: 184.319992\n",
            "Epoch 9, Batch 490, Loss: 184.6555, Main: 0.3355, Load Balance: 184.319992\n",
            "Epoch 9, Batch 500, Loss: 184.7989, Main: 0.4789, Load Balance: 184.319992\n",
            "Epoch 9, Batch 510, Loss: 184.7407, Main: 0.4207, Load Balance: 184.319992\n",
            "Epoch 9, Batch 520, Loss: 184.6477, Main: 0.3277, Load Balance: 184.319992\n",
            "Epoch 9, Batch 530, Loss: 184.7001, Main: 0.3801, Load Balance: 184.319992\n",
            "Epoch 9, Batch 540, Loss: 184.7482, Main: 0.4282, Load Balance: 184.319992\n",
            "Epoch 9, Batch 550, Loss: 184.7318, Main: 0.4118, Load Balance: 184.319992\n",
            "Epoch 9, Batch 560, Loss: 184.7278, Main: 0.4078, Load Balance: 184.319992\n",
            "Epoch 9, Batch 570, Loss: 184.8696, Main: 0.5496, Load Balance: 184.319992\n",
            "Epoch 9, Batch 580, Loss: 184.6951, Main: 0.3751, Load Balance: 184.319992\n",
            "Epoch 9, Batch 590, Loss: 184.6934, Main: 0.3734, Load Balance: 184.319992\n",
            "Epoch 9, Batch 600, Loss: 184.8053, Main: 0.4853, Load Balance: 184.319992\n",
            "Epoch 9 completed. Avg Loss: 0.4346, Avg LB Loss: 184.151015\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating sample text:\n",
            "The telkkkkententl kkkkkkkkkkkkkkkkkkententlkentlke kkkkkkentlkentlkentlkentllllllMiokencicicokenciokadintencincincincincincincincincincincincincncncncncntentencompetecincicicicincicicicincicicicicincinci\n",
            "\n",
            "Expert usage statistics:\n",
            "Expert 0: 800 tokens\n",
            "Expert 1: 800 tokens\n",
            "Expert 2: 800 tokens\n",
            "Expert 3: 800 tokens\n",
            "\n",
            "Training completed successfully!\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Sparse Mixture-of-Experts with Expert Choice Routing\n",
        "Implementation for research and educational purposes\n",
        "\n",
        "This implementation includes:\n",
        "1. Expert Choice Routing mechanism\n",
        "2. Load balancing\n",
        "3. Training utilities\n",
        "4. Example usage with language modeling\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Tuple, Optional, List\n",
        "import random\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "class ExpertChoiceRouter(nn.Module):\n",
        "    \"\"\"\n",
        "    Expert Choice Router: Each expert selects top-k tokens to process\n",
        "    instead of tokens selecting experts.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, num_experts: int, expert_capacity: int):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_experts = num_experts\n",
        "        self.expert_capacity = expert_capacity\n",
        "\n",
        "        # Router network - produces expert preferences for tokens\n",
        "        self.router = nn.Linear(d_model, num_experts, bias=False)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor [batch_size, seq_len, d_model]\n",
        "\n",
        "        Returns:\n",
        "            expert_weights: Router weights [batch_size, seq_len, num_experts]\n",
        "            expert_indices: Which tokens each expert processes [num_experts, expert_capacity]\n",
        "            token_to_expert: Mapping from tokens to experts [batch_size, seq_len]\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "\n",
        "        # Get router logits for each token\n",
        "        router_logits = self.router(x)  # [batch_size, seq_len, num_experts]\n",
        "        router_probs = F.softmax(router_logits, dim=-1)\n",
        "\n",
        "        # Flatten for expert choice\n",
        "        flat_probs = router_probs.view(-1, self.num_experts)  # [batch_size*seq_len, num_experts]\n",
        "\n",
        "        # Expert Choice: Each expert picks top-k tokens\n",
        "        expert_indices = torch.zeros(self.num_experts, self.expert_capacity, dtype=torch.long, device=x.device)\n",
        "        expert_weights = torch.zeros(self.num_experts, self.expert_capacity, device=x.device)\n",
        "        token_to_expert = torch.full((batch_size * seq_len,), -1, dtype=torch.long, device=x.device)\n",
        "\n",
        "        for expert_id in range(self.num_experts):\n",
        "            # Get this expert's preferences for all tokens\n",
        "            expert_scores = flat_probs[:, expert_id]  # [batch_size*seq_len]\n",
        "\n",
        "            # Select top-k tokens for this expert\n",
        "            top_values, top_indices = torch.topk(expert_scores,\n",
        "                                               min(self.expert_capacity, len(expert_scores)),\n",
        "                                               dim=0)\n",
        "\n",
        "            # Store the selected tokens and their weights\n",
        "            actual_capacity = len(top_indices)\n",
        "            expert_indices[expert_id, :actual_capacity] = top_indices\n",
        "            expert_weights[expert_id, :actual_capacity] = top_values\n",
        "\n",
        "            # Mark these tokens as assigned to this expert\n",
        "            token_to_expert[top_indices] = expert_id\n",
        "\n",
        "        # Reshape router_probs back to original shape for compatibility\n",
        "        return router_probs, expert_indices, token_to_expert\n",
        "\n",
        "class MoEExpert(nn.Module):\n",
        "    \"\"\"Individual expert network (simple FFN)\"\"\"\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.w1 = nn.Linear(d_model, d_ff)\n",
        "        self.w2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.w2(self.dropout(F.relu(self.w1(x))))\n",
        "\n",
        "class SparseExpertChoiceMoE(nn.Module):\n",
        "    \"\"\"\n",
        "    Sparse Mixture-of-Experts with Expert Choice Routing\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, num_experts: int, d_ff: int,\n",
        "                 expert_capacity: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.num_experts = num_experts\n",
        "        self.expert_capacity = expert_capacity\n",
        "\n",
        "        # Router\n",
        "        self.router = ExpertChoiceRouter(d_model, num_experts, expert_capacity)\n",
        "\n",
        "        # Experts\n",
        "        self.experts = nn.ModuleList([\n",
        "            MoEExpert(d_model, d_ff, dropout) for _ in range(num_experts)\n",
        "        ])\n",
        "\n",
        "        # Load balancing\n",
        "        self.load_balance_loss_coeff = 0.01\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor [batch_size, seq_len, d_model]\n",
        "\n",
        "        Returns:\n",
        "            output: MoE output [batch_size, seq_len, d_model]\n",
        "            load_balance_loss: Load balancing auxiliary loss\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, d_model = x.shape\n",
        "\n",
        "        # Get routing decisions\n",
        "        router_probs, expert_indices, token_to_expert = self.router(x)\n",
        "\n",
        "        # Flatten input for expert processing\n",
        "        flat_x = x.view(-1, d_model)  # [batch_size*seq_len, d_model]\n",
        "\n",
        "        # Initialize output\n",
        "        output = torch.zeros_like(flat_x)\n",
        "\n",
        "        # Process tokens through their assigned experts\n",
        "        for expert_id, expert in enumerate(self.experts):\n",
        "            # Get tokens assigned to this expert\n",
        "            expert_token_indices = expert_indices[expert_id]\n",
        "            expert_weights_for_tokens = self.router.router(flat_x)[expert_token_indices, expert_id]\n",
        "\n",
        "            # Filter out padding (where index might be 0 but not actually assigned)\n",
        "            mask = expert_token_indices < batch_size * seq_len\n",
        "            valid_indices = expert_token_indices[mask]\n",
        "            valid_weights = expert_weights_for_tokens[mask]\n",
        "\n",
        "            if len(valid_indices) > 0:\n",
        "                # Get input tokens for this expert\n",
        "                expert_input = flat_x[valid_indices]  # [num_selected_tokens, d_model]\n",
        "\n",
        "                # Process through expert\n",
        "                expert_output = expert(expert_input)  # [num_selected_tokens, d_model]\n",
        "\n",
        "                # Weight the expert output\n",
        "                weighted_output = expert_output * valid_weights.unsqueeze(-1)\n",
        "\n",
        "                # Add to final output\n",
        "                output[valid_indices] += weighted_output\n",
        "\n",
        "        # Reshape back to original dimensions\n",
        "        output = output.view(batch_size, seq_len, d_model)\n",
        "\n",
        "        # Calculate load balancing loss\n",
        "        load_balance_loss = self._calculate_load_balance_loss(router_probs, expert_indices)\n",
        "\n",
        "        return output, load_balance_loss\n",
        "\n",
        "    def _calculate_load_balance_loss(self, router_probs: torch.Tensor,\n",
        "                                   expert_indices: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Calculate load balancing auxiliary loss\"\"\"\n",
        "        # Expert utilization (how many tokens each expert processes)\n",
        "        expert_counts = torch.zeros(self.num_experts, device=router_probs.device)\n",
        "        for expert_id in range(self.num_experts):\n",
        "            expert_counts[expert_id] = (expert_indices[expert_id] >= 0).sum().float()\n",
        "\n",
        "        # Target: uniform distribution\n",
        "        total_tokens = router_probs.shape[0] * router_probs.shape[1]\n",
        "        target_count = total_tokens / self.num_experts\n",
        "\n",
        "        # Load balance loss: encourage uniform expert utilization\n",
        "        load_balance_loss = F.mse_loss(expert_counts,\n",
        "                                     torch.full_like(expert_counts, target_count))\n",
        "\n",
        "        return self.load_balance_loss_coeff * load_balance_loss\n",
        "\n",
        "class TransformerWithMoE(nn.Module):\n",
        "    \"\"\"Simple transformer with MoE layer for demonstration\"\"\"\n",
        "    def __init__(self, vocab_size: int, d_model: int, nhead: int, num_layers: int,\n",
        "                 num_experts: int, d_ff: int, expert_capacity: int, max_seq_len: int = 512):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Embeddings\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(max_seq_len, d_model))\n",
        "\n",
        "        # Transformer layers\n",
        "        self.layers = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            # Attention layer\n",
        "            attention = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
        "            # MoE layer\n",
        "            moe = SparseExpertChoiceMoE(d_model, num_experts, d_ff, expert_capacity)\n",
        "            # Layer norms\n",
        "            norm1 = nn.LayerNorm(d_model)\n",
        "            norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "            self.layers.append(nn.ModuleDict({\n",
        "                'attention': attention,\n",
        "                'moe': moe,\n",
        "                'norm1': norm1,\n",
        "                'norm2': norm2\n",
        "            }))\n",
        "\n",
        "        # Output projection\n",
        "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        # Embeddings\n",
        "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "        x = x + self.pos_embedding[:seq_len]\n",
        "\n",
        "        total_load_balance_loss = 0\n",
        "\n",
        "        # Transformer layers\n",
        "        for layer in self.layers:\n",
        "            # Self-attention\n",
        "            residual = x\n",
        "            x = layer['norm1'](x)\n",
        "            attn_output, _ = layer['attention'](x, x, x, attn_mask=mask)\n",
        "            x = residual + attn_output\n",
        "\n",
        "            # MoE\n",
        "            residual = x\n",
        "            x = layer['norm2'](x)\n",
        "            moe_output, load_balance_loss = layer['moe'](x)\n",
        "            x = residual + moe_output\n",
        "            total_load_balance_loss += load_balance_loss\n",
        "\n",
        "        # Output\n",
        "        logits = self.output_projection(x)\n",
        "\n",
        "        return logits, total_load_balance_loss\n",
        "\n",
        "# Example dataset for language modeling\n",
        "class SimpleTextDataset(Dataset):\n",
        "    \"\"\"Simple character-level dataset for demonstration\"\"\"\n",
        "    def __init__(self, text: str, seq_len: int = 128):\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        # Create character vocabulary\n",
        "        self.chars = sorted(list(set(text)))\n",
        "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
        "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
        "        self.vocab_size = len(self.chars)\n",
        "\n",
        "        # Encode text\n",
        "        self.data = [self.char_to_idx[ch] for ch in text]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (torch.tensor(self.data[idx:idx + self.seq_len], dtype=torch.long),\n",
        "                torch.tensor(self.data[idx + 1:idx + self.seq_len + 1], dtype=torch.long))\n",
        "\n",
        "def train_model():\n",
        "    \"\"\"Training loop example\"\"\"\n",
        "    # Sample text data (you can replace with your own dataset)\n",
        "    sample_text = \"\"\"\n",
        "    The field of artificial intelligence has seen remarkable progress in recent years,\n",
        "    particularly in the area of large language models. Mixture-of-experts architectures\n",
        "    represent an important advancement that allows models to scale efficiently by\n",
        "    activating only a subset of parameters for each input. Expert choice routing\n",
        "    further improves this by allowing experts to select which tokens to process,\n",
        "    leading to better load balancing and computational efficiency.\n",
        "    \"\"\" * 10  # Repeat for more training data\n",
        "\n",
        "    # Hyperparameters\n",
        "    vocab_size = len(set(sample_text))\n",
        "    d_model = 128\n",
        "    nhead = 4\n",
        "    num_layers = 2\n",
        "    num_experts = 4\n",
        "    d_ff = 256\n",
        "    expert_capacity = 32\n",
        "    seq_len = 64\n",
        "    batch_size = 8\n",
        "    learning_rate = 1e-3\n",
        "    num_epochs = 10\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = SimpleTextDataset(sample_text, seq_len)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Create model\n",
        "    model = TransformerWithMoE(\n",
        "        vocab_size=dataset.vocab_size,\n",
        "        d_model=d_model,\n",
        "        nhead=nhead,\n",
        "        num_layers=num_layers,\n",
        "        num_experts=num_experts,\n",
        "        d_ff=d_ff,\n",
        "        expert_capacity=expert_capacity\n",
        "    ).to(device)\n",
        "\n",
        "    print(f\"Model has {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Training loop\n",
        "    model.train()\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0\n",
        "        epoch_load_balance_loss = 0\n",
        "\n",
        "        for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            logits, load_balance_loss = model(inputs)\n",
        "\n",
        "            # Calculate main loss (cross-entropy)\n",
        "            main_loss = F.cross_entropy(logits.reshape(-1, dataset.vocab_size), targets.reshape(-1))\n",
        "\n",
        "            # Total loss\n",
        "            total_loss = main_loss + load_balance_loss\n",
        "\n",
        "            # Backward pass\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += main_loss.item()\n",
        "            epoch_load_balance_loss += load_balance_loss.item()\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                print(f'Epoch {epoch}, Batch {batch_idx}, Loss: {total_loss.item():.4f}, '\n",
        "                      f'Main: {main_loss.item():.4f}, Load Balance: {load_balance_loss.item():.6f}')\n",
        "\n",
        "        avg_loss = epoch_loss / len(dataloader)\n",
        "        avg_lb_loss = epoch_load_balance_loss / len(dataloader)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "        print(f'Epoch {epoch} completed. Avg Loss: {avg_loss:.4f}, Avg LB Loss: {avg_lb_loss:.6f}')\n",
        "\n",
        "    return model, losses, dataset\n",
        "\n",
        "def generate_text(model, dataset, seed_text: str, length: int = 100):\n",
        "    \"\"\"Generate text using the trained model\"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    # Convert seed text to indices\n",
        "    seed_indices = [dataset.char_to_idx.get(ch, 0) for ch in seed_text]\n",
        "    generated = seed_indices.copy()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(length):\n",
        "            # Take last seq_len tokens as input\n",
        "            input_seq = generated[-64:] if len(generated) >= 64 else generated\n",
        "            input_tensor = torch.tensor([input_seq], device=device)\n",
        "\n",
        "            # Get prediction\n",
        "            logits, _ = model(input_tensor)\n",
        "            next_token_logits = logits[0, -1, :]\n",
        "\n",
        "            # Sample next token (with temperature)\n",
        "            temperature = 0.8\n",
        "            probs = F.softmax(next_token_logits / temperature, dim=-1)\n",
        "            next_token = torch.multinomial(probs, 1).item()\n",
        "\n",
        "            generated.append(next_token)\n",
        "\n",
        "    # Convert back to text\n",
        "    return ''.join([dataset.idx_to_char[idx] for idx in generated])\n",
        "\n",
        "# Example usage and analysis\n",
        "def analyze_expert_usage(model, dataset, num_samples: int = 100):\n",
        "    \"\"\"Analyze how experts are being used\"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    expert_usage = {i: 0 for i in range(model.layers[0]['moe'].num_experts)}\n",
        "\n",
        "    # Create some sample inputs\n",
        "    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, _) in enumerate(dataloader):\n",
        "            if batch_idx >= num_samples // 4:\n",
        "                break\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "\n",
        "            # Get routing information from first MoE layer\n",
        "            x = model.embedding(inputs) * math.sqrt(model.d_model)\n",
        "            x = x + model.pos_embedding[:inputs.shape[1]]\n",
        "\n",
        "            # Process through first layer to get MoE routing\n",
        "            layer = model.layers[0]\n",
        "            x = layer['norm1'](x)\n",
        "            attn_output, _ = layer['attention'](x, x, x)\n",
        "            x = x + attn_output\n",
        "            x = layer['norm2'](x)\n",
        "\n",
        "            # Get routing decisions\n",
        "            router_probs, expert_indices, token_to_expert = layer['moe'].router(x)\n",
        "\n",
        "            # Count expert usage\n",
        "            for expert_id in range(model.layers[0]['moe'].num_experts):\n",
        "                expert_usage[expert_id] += (expert_indices[expert_id] >= 0).sum().item()\n",
        "\n",
        "    return expert_usage\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Training Sparse MoE with Expert Choice Routing...\")\n",
        "\n",
        "    # Train the model\n",
        "    model, losses, dataset = train_model()\n",
        "\n",
        "    # Plot training losses\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(losses)\n",
        "    plt.title('Training Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    # Analyze expert usage\n",
        "    expert_usage = analyze_expert_usage(model, dataset)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    experts = list(expert_usage.keys())\n",
        "    usage_counts = list(expert_usage.values())\n",
        "    plt.bar(experts, usage_counts)\n",
        "    plt.title('Expert Usage Distribution')\n",
        "    plt.xlabel('Expert ID')\n",
        "    plt.ylabel('Number of Tokens Processed')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Generate some sample text\n",
        "    print(\"\\nGenerating sample text:\")\n",
        "    sample_generation = generate_text(model, dataset, \"The \", length=200)\n",
        "    print(sample_generation)\n",
        "\n",
        "    print(f\"\\nExpert usage statistics:\")\n",
        "    for expert_id, count in expert_usage.items():\n",
        "        print(f\"Expert {expert_id}: {count} tokens\")\n",
        "\n",
        "    print(\"\\nTraining completed successfully!\")"
      ]
    }
  ]
}